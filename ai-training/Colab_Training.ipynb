{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”¥ Have I Been Rekt - Threat Intelligence Training\n",
        "## Safe overnight training for your 6-year-old GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean setup\n",
        "import os\n",
        "if os.path.exists('Have-I-Been-Rekt'):\n",
        "    !rm -rf Have-I-Been-Rekt\n",
        "    \n",
        "!git clone https://github.com/Pretty-Good-OSINT-Protocol/Have-I-Been-Rekt.git\n",
        "%cd Have-I-Been-Rekt/ai-training\n",
        "!ls -la datasets/\n",
        "print('Repository cloned successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers torch datasets scikit-learn pandas numpy\n",
        "import torch\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "print('Ready for training!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load existing threat data\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('datasets/comprehensive_threat_intelligence.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f'Loaded {len(data)} threat intelligence records')\n",
        "\n",
        "# Show sample data\n",
        "for i, record in enumerate(data[:3]):\n",
        "    print(f'Sample {i+1}: {record[\"type\"]} - {list(record[\"data\"].keys())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for record in data:\n",
        "    content = record.get('data', {})\n",
        "    data_type = record.get('type', '')\n",
        "    \n",
        "    if 'username' in data_type:\n",
        "        username = content.get('username', '')\n",
        "        scams = len(content.get('scam_reports', []))\n",
        "        text = f'Username {username} has {scams} scam reports'\n",
        "        label = 1 if scams > 0 else 0\n",
        "    elif 'domain' in data_type:\n",
        "        domain = content.get('domain', '')\n",
        "        phishing = len(content.get('phishing_indicators', []))\n",
        "        text = f'Domain {domain} has {phishing} phishing indicators'\n",
        "        label = 1 if phishing > 0 else 0\n",
        "    else:\n",
        "        text = f'Data type {data_type}'\n",
        "        label = 0\n",
        "    \n",
        "    texts.append(text)\n",
        "    labels.append(label)\n",
        "\n",
        "print(f'Prepared {len(texts)} training samples')\n",
        "print('Sample texts:')\n",
        "for i, text in enumerate(texts[:3]):\n",
        "    print(f'  {i+1}. {text} (label: {labels[i]})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset class\n",
        "class ThreatDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize small model for overnight training\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "print('Model and tokenizer loaded')\n",
        "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expand dataset with synthetic examples\n",
        "synthetic_texts = [\n",
        "    'Username admin_help has 1 scam reports',\n",
        "    'Username normal_user has 0 scam reports', \n",
        "    'Domain legitimate-site.com has 0 phishing indicators',\n",
        "    'Domain fake-exchange.tk has 2 phishing indicators',\n",
        "    'Username crypto_scammer has 1 scam reports',\n",
        "    'Domain binance-help.org has 1 phishing indicators'\n",
        "]\n",
        "\n",
        "synthetic_labels = [1, 0, 0, 1, 1, 1]\n",
        "\n",
        "# Combine with real data\n",
        "all_texts = texts + synthetic_texts\n",
        "all_labels = labels + synthetic_labels\n",
        "\n",
        "print(f'Total training data: {len(all_texts)} samples')\n",
        "print(f'Positive samples: {sum(all_labels)}')\n",
        "print(f'Negative samples: {len(all_labels) - sum(all_labels)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split and create datasets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    all_texts, all_labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = ThreatDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = ThreatDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "print(f'Training: {len(train_dataset)} samples')\n",
        "print(f'Validation: {len(val_dataset)} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration optimized for overnight runs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./threat-model',\n",
        "    num_train_epochs=20,  # Perfect for overnight\n",
        "    per_device_train_batch_size=4,  # Small batch for older GPUs\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=5,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,  # Mixed precision for speed\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "print('Trainer configured for overnight run!')\n",
        "print('Configuration:')\n",
        "print(f'  - Epochs: {training_args.num_train_epochs}')\n",
        "print(f'  - Batch size: {training_args.per_device_train_batch_size}')\n",
        "print(f'  - Mixed precision: {training_args.fp16}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# START TRAINING - Perfect for overnight runs!\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "print('ðŸš€ STARTING THREAT INTELLIGENCE TRAINING')\n",
        "print('=' * 50)\n",
        "print(f'Start time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "print('Optimized for overnight training on older GPUs')\n",
        "print('=' * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    result = trainer.train()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    duration = (end_time - start_time) / 3600  # hours\n",
        "    \n",
        "    print('\\n' + '=' * 50)\n",
        "    print('ðŸŽ‰ TRAINING COMPLETED!')\n",
        "    print('=' * 50)\n",
        "    print(f'Duration: {duration:.2f} hours')\n",
        "    print(f'Final loss: {result.training_loss:.4f}')\n",
        "    print(f'End time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f'Training error: {e}')\n",
        "    print('Saving current progress...')\n",
        "    trainer.save_model('./backup-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "trainer.save_model('./final-threat-model')\n",
        "tokenizer.save_pretrained('./final-threat-model')\n",
        "\n",
        "# Quick test\n",
        "test_cases = [\n",
        "    'Username crypto_king_official has 1 scam reports',\n",
        "    'Username john_doe has 0 scam reports',\n",
        "    'Domain fake-metamask.org has 3 phishing indicators',\n",
        "    'Domain google.com has 0 phishing indicators'\n",
        "]\n",
        "\n",
        "print('ðŸ§ª TESTING TRAINED MODEL')\n",
        "print('=' * 40)\n",
        "\n",
        "model.eval()\n",
        "for i, test_text in enumerate(test_cases):\n",
        "    inputs = tokenizer(test_text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        threat_score = prediction[0][1].item()  # Probability of being a threat\n",
        "        \n",
        "    threat_level = 'HIGH' if threat_score > 0.7 else 'MEDIUM' if threat_score > 0.3 else 'LOW'\n",
        "    print(f'{i+1}. {test_text[:40]}...')\n",
        "    print(f'   Threat: {threat_level} ({threat_score:.3f})')\n",
        "    print()\n",
        "\n",
        "print('âœ… MODEL TRAINING COMPLETE!')\n",
        "print('ðŸ’¾ Model saved to ./final-threat-model')\n",
        "print('ðŸŽ¯ Ready for integration with Have I Been Rekt!')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}