{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”¥ Comprehensive Threat Intelligence Training\n",
        "## Optimized for overnight runs on older hardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "!git clone https://github.com/Pretty-Good-OSINT-Protocol/Have-I-Been-Rekt.git\n",
        "%cd Have-I-Been-Rekt/ai-training\n",
        "!pip install -q transformers torch datasets accelerate scikit-learn pandas numpy\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate threat intelligence data\n",
        "!python3 collect_comprehensive_intelligence.py\n",
        "print(\"âœ… Data generated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "print(f\"ðŸ”¥ Using: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "with open('datasets/comprehensive_threat_intelligence.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"ðŸ“Š Loaded {len(data)} threat intelligence records\")\n",
        "\n",
        "# Prepare training data\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for record in data:\n",
        "    data_type = record.get('type', '')\n",
        "    content = record.get('data', {})\n",
        "    \n",
        "    # Create text representation\n",
        "    if data_type == 'username_intelligence':\n",
        "        username = content.get('username', '')\n",
        "        scams = len(content.get('scam_reports', []))\n",
        "        text = f\"Username: {username} Scam reports: {scams}\"\n",
        "        label = 2 if scams > 0 else 0\n",
        "    elif data_type == 'domain_intelligence':\n",
        "        domain = content.get('domain', '')\n",
        "        phishing = len(content.get('phishing_indicators', []))\n",
        "        text = f\"Domain: {domain} Phishing indicators: {phishing}\"\n",
        "        label = 2 if phishing > 0 else 0\n",
        "    else:\n",
        "        text = f\"Type: {data_type}\"\n",
        "        label = 0\n",
        "    \n",
        "    texts.append(text)\n",
        "    labels.append(label)\n",
        "\n",
        "print(f\"âœ… Prepared {len(texts)} training samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "class ThreatDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Setup model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "print(\"âœ… Model and tokenizer loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = ThreatDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = ThreatDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "print(f\"ðŸ“Š Training: {len(train_dataset)}, Validation: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration (optimized for overnight run)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./models/threat-intelligence',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting training - perfect for overnight run!\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save and test\n",
        "trainer.save_model('./final-model')\n",
        "tokenizer.save_pretrained('./final-model')\n",
        "\n",
        "# Test prediction\n",
        "test_text = \"Username: @crypto_king_2024 Scam reports: 1\"\n",
        "inputs = tokenizer(test_text, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    \n",
        "print(f\"ðŸ§ª Test prediction: {prediction}\")\n",
        "print(\"ðŸŽ‰ Your threat intelligence model is ready!\")\n",
        "print(\"ðŸ’¾ Model saved to ./final-model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}