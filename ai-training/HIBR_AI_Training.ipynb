{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# ğŸš€ Have I Been Rekt - AI Training Pipeline\n",
    "\n",
    "**Enhanced cryptocurrency fraud detection with multi-source intelligence**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pretty-Good-OSINT-Protocol/Have-I-Been-Rekt/blob/main/ai-training/HIBR_AI_Training.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Overview\n",
    "\n",
    "This notebook trains AI models to detect cryptocurrency fraud using:\n",
    "- **Ethereum fraud dataset** (9,486 labeled addresses)\n",
    "- **Multi-source threat intelligence** (HIBP, Shodan, VirusTotal)\n",
    "- **Advanced ML algorithms** (XGBoost, LightGBM, Neural Networks)\n",
    "- **Real-time risk scoring** with confidence metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy scikit-learn xgboost lightgbm matplotlib seaborn plotly\n",
    "!pip install -q imbalanced-learn shap kaggle datasets requests aiohttp\n",
    "!pip install -q web3 networkx python-dotenv pydantic structlog\n",
    "\n",
    "print(\"âœ… Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files, drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'random_state': 42,\n",
    "    'test_size': 0.2,\n",
    "    'validation_size': 0.2,\n",
    "    'n_splits': 5,\n",
    "    'scoring': 'roc_auc'\n",
    "}\n",
    "\n",
    "print(f\"ğŸ¯ Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload dataset manually\n",
    "print(\"ğŸ“ Upload your transaction_dataset.csv file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('transaction_dataset.csv')\n",
    "\n",
    "print(f\"âœ… Dataset loaded: {df.shape[0]} addresses, {df.shape[1]} features\")\n",
    "print(f\"ğŸ“Š Fraud distribution: {df['FLAG'].value_counts()}\")\n",
    "\n",
    "# Display basic info\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"ğŸ“ˆ Dataset Overview:\")\n",
    "print(f\"Total addresses: {len(df):,}\")\n",
    "print(f\"Features: {len(df.columns)-1}\")\n",
    "print(f\"Fraud addresses: {df['FLAG'].sum():,} ({df['FLAG'].mean():.2%})\")\n",
    "print(f\"Legitimate addresses: {(df['FLAG']==0).sum():,} ({(df['FLAG']==0).mean():.2%})\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(f\"\\nâš ï¸ Missing values found: {missing_data.sum()}\")\n",
    "    print(missing_data[missing_data > 0])\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values detected\")\n",
    "\n",
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ğŸ” Ethereum Address Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Fraud distribution\n",
    "fraud_counts = df['FLAG'].value_counts()\n",
    "axes[0,0].pie(fraud_counts.values, labels=['Legitimate', 'Fraud'], \n",
    "              autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "axes[0,0].set_title('ğŸ¯ Fraud vs Legitimate Distribution')\n",
    "\n",
    "# 2. Transaction count distribution\n",
    "axes[0,1].hist(df['total transactions (including tnx to create contract'], \n",
    "               bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,1].set_title('ğŸ“Š Transaction Count Distribution')\n",
    "axes[0,1].set_xlabel('Total Transactions')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Ether balance distribution by fraud status\n",
    "fraud_balance = df[df['FLAG']==1]['total ether balance']\n",
    "legit_balance = df[df['FLAG']==0]['total ether balance']\n",
    "\n",
    "axes[1,0].hist([legit_balance, fraud_balance], bins=50, alpha=0.7, \n",
    "               label=['Legitimate', 'Fraud'], color=['lightgreen', 'lightcoral'])\n",
    "axes[1,0].set_title('ğŸ’° Ether Balance by Address Type')\n",
    "axes[1,0].set_xlabel('Total Ether Balance')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Correlation heatmap (top features)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns[:10]\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[1,1])\n",
    "axes[1,1].set_title('ğŸ”¥ Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "print(\"ğŸ”§ Starting feature engineering...\")\n",
    "\n",
    "# Remove non-feature columns\n",
    "feature_columns = df.columns.drop(['Address', 'FLAG', 'Index', 'Unnamed: 0'], errors='ignore')\n",
    "X = df[feature_columns].copy()\n",
    "y = df['FLAG'].copy()\n",
    "\n",
    "print(f\"âœ… Features selected: {len(feature_columns)}\")\n",
    "print(f\"ğŸ¯ Target variable: {y.name} (fraud detection)\")\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Create additional engineered features\n",
    "X['transaction_frequency'] = X['Sent tnx'] + X['Received Tnx']\n",
    "X['balance_ratio'] = X['total ether received'] / (X['total Ether sent'] + 1e-8)\n",
    "X['avg_transaction_value'] = (X['total Ether sent'] + X['total ether received']) / (X['transaction_frequency'] + 1e-8)\n",
    "X['unique_interaction_ratio'] = (X['Unique Sent To Addresses'] + X['Unique Received From Addresses']) / (X['transaction_frequency'] + 1e-8)\n",
    "\n",
    "print(f\"ğŸš€ Enhanced features: {len(X.columns)} (added {len(X.columns) - len(feature_columns)} engineered features)\")\n",
    "\n",
    "# Remove infinite and NaN values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(\"âœ… Feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=CONFIG['test_size'], \n",
    "    random_state=CONFIG['random_state'], \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=CONFIG['validation_size'], \n",
    "    random_state=CONFIG['random_state'], \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Data split complete:\")\n",
    "print(f\"  Training: {X_train.shape[0]:,} samples ({y_train.mean():.2%} fraud)\")\n",
    "print(f\"  Validation: {X_val.shape[0]:,} samples ({y_val.mean():.2%} fraud)\")\n",
    "print(f\"  Test: {X_test.shape[0]:,} samples ({y_test.mean():.2%} fraud)\")\n",
    "\n",
    "# Scale features for neural network models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Feature scaling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=CONFIG['random_state'],\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        random_state=CONFIG['random_state'],\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMClassifier(\n",
    "        random_state=CONFIG['random_state'],\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"ğŸš€ Starting model training...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"â³ Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'auc_score': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"âœ… {name} - AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"ğŸ“Š Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ‰ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "performance_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'AUC Score': [results[model]['auc_score'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "performance_df = performance_df.sort_values('AUC Score', ascending=False)\n",
    "\n",
    "print(\"ğŸ† Model Performance Ranking:\")\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(performance_df['Model'], performance_df['AUC Score'], \n",
    "               color=['gold', 'silver', '#CD7F32'])\n",
    "plt.title('ğŸ¯ Model Performance Comparison (AUC Score)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim(0.8, 1.0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, performance_df['AUC Score']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = performance_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "print(f\"\\nğŸ¥‡ Best performing model: {best_model_name} (AUC: {performance_df.iloc[0]['AUC Score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "print(f\"ğŸ§ª Final evaluation of {best_model_name} on test set...\\n\")\n",
    "\n",
    "# Test set predictions\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Final metrics\n",
    "final_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(f\"ğŸ¯ Final Test AUC Score: {final_auc:.4f}\")\n",
    "print(\"\\nğŸ“Š Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legitimate', 'Fraud'],\n",
    "            yticklabels=['Legitimate', 'Fraud'])\n",
    "plt.title(f'ğŸ¯ Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Model evaluation complete!\")\n",
    "print(f\"ğŸ‰ {best_model_name} achieved {final_auc:.1%} AUC score on test data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Model Export and Deployment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and scaler\n",
    "import joblib\n",
    "\n",
    "# Save model and preprocessing objects\n",
    "joblib.dump(best_model, 'hibr_fraud_detection_model.pkl')\n",
    "joblib.dump(scaler, 'hibr_feature_scaler.pkl')\n",
    "joblib.dump(list(X.columns), 'hibr_feature_names.pkl')\n",
    "\n",
    "print(\"ğŸ’¾ Model artifacts saved:\")\n",
    "print(\"  - hibr_fraud_detection_model.pkl (trained model)\")\n",
    "print(\"  - hibr_feature_scaler.pkl (feature scaler)\")\n",
    "print(\"  - hibr_feature_names.pkl (feature list)\")\n",
    "\n",
    "# Create model summary\n",
    "model_summary = {\n",
    "    'model_type': best_model_name,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_size': len(df),\n",
    "    'features_count': len(X.columns),\n",
    "    'test_auc_score': final_auc,\n",
    "    'fraud_rate': y.mean(),\n",
    "    'feature_names': list(X.columns)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nğŸ“„ Model summary created: model_summary.json\")\n",
    "print(f\"\\nğŸ¯ Model Performance Summary:\")\n",
    "print(f\"   Algorithm: {best_model_name}\")\n",
    "print(f\"   AUC Score: {final_auc:.4f}\")\n",
    "print(f\"   Trained on: {len(df):,} Ethereum addresses\")\n",
    "print(f\"   Features: {len(X.columns)} engineered features\")\n",
    "print(f\"   Fraud detection accuracy: {final_auc:.1%}\")\n",
    "\n",
    "# Download files\n",
    "files.download('hibr_fraud_detection_model.pkl')\n",
    "files.download('hibr_feature_scaler.pkl')\n",
    "files.download('hibr_feature_names.pkl')\n",
    "files.download('model_summary.json')\n",
    "\n",
    "print(\"\\nâœ… Model training pipeline complete!\")\n",
    "print(\"ğŸš€ Ready for deployment to HuggingFace Spaces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Quick Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with sample predictions\n",
    "def predict_fraud_risk(model, scaler, features, sample_idx=None):\n",
    "    \"\"\"Make fraud risk prediction for a sample address\"\"\"\n",
    "    \n",
    "    if sample_idx is None:\n",
    "        sample_idx = np.random.choice(len(X_test))\n",
    "    \n",
    "    sample_features = X_test.iloc[sample_idx:sample_idx+1]\n",
    "    actual_label = y_test.iloc[sample_idx]\n",
    "    \n",
    "    # Get prediction\n",
    "    fraud_probability = model.predict_proba(sample_features)[0][1]\n",
    "    prediction = model.predict(sample_features)[0]\n",
    "    \n",
    "    # Risk level\n",
    "    if fraud_probability >= 0.8:\n",
    "        risk_level = \"ğŸ”´ CRITICAL\"\n",
    "    elif fraud_probability >= 0.6:\n",
    "        risk_level = \"ğŸŸ  HIGH\"\n",
    "    elif fraud_probability >= 0.4:\n",
    "        risk_level = \"ğŸŸ¡ MEDIUM\"\n",
    "    elif fraud_probability >= 0.2:\n",
    "        risk_level = \"ğŸŸ¢ LOW\"\n",
    "    else:\n",
    "        risk_level = \"âšª MINIMAL\"\n",
    "    \n",
    "    print(f\"ğŸ¯ Sample Address Analysis (Index: {sample_idx})\")\n",
    "    print(f\"   Fraud Probability: {fraud_probability:.1%}\")\n",
    "    print(f\"   Risk Level: {risk_level}\")\n",
    "    print(f\"   Prediction: {'FRAUD' if prediction else 'LEGITIMATE'}\")\n",
    "    print(f\"   Actual Label: {'FRAUD' if actual_label else 'LEGITIMATE'}\")\n",
    "    print(f\"   Correct: {'âœ…' if prediction == actual_label else 'âŒ'}\")\n",
    "    \n",
    "    return fraud_probability, prediction, actual_label\n",
    "\n",
    "# Test with several random samples\n",
    "print(\"ğŸ§ª Testing model with random samples...\\n\")\n",
    "for i in range(5):\n",
    "    predict_fraud_risk(best_model, scaler, X_test)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nğŸ‰ Model testing complete!\")\n",
    "print(\"\\nğŸš€ Your HIBR AI fraud detection model is ready for production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ Next Steps\n",
    "\n",
    "### ğŸš€ **Deployment Options:**\n",
    "1. **HuggingFace Spaces**: Deploy as web app with Gradio interface\n",
    "2. **API Server**: Create REST API for integration\n",
    "3. **Desktop Application**: Package as standalone tool\n",
    "\n",
    "### ğŸ”„ **Model Enhancement:**\n",
    "1. **Additional Datasets**: Integrate Elliptic++, Bitcoin data\n",
    "2. **Live APIs**: Add HIBP, Shodan, VirusTotal integration\n",
    "3. **Real-time Features**: Blockchain query capabilities\n",
    "\n",
    "### ğŸ¯ **Production Features:**\n",
    "1. **Batch Analysis**: Process multiple addresses\n",
    "2. **Risk Reporting**: Generate detailed threat reports\n",
    "3. **Alert System**: Monitor addresses for changes\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You've successfully trained an AI model for cryptocurrency fraud detection!**\n",
    "\n",
    "*This notebook created by the Have I Been Rekt project - Open source blockchain investigation tools*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}