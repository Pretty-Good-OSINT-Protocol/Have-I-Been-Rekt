{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comprehensive-threat-header"
      },
      "source": [
        "# 🔥 Comprehensive Threat Intelligence AI Training\n",
        "## Multi-Modal Blockchain + OSINT Analysis\n",
        "\n",
        "**Optimized for overnight training runs** 🌙\n",
        "\n",
        "This notebook trains an AI model on:\n",
        "- 🔗 Blockchain addresses and transaction patterns  \n",
        "- 👤 Usernames and social media handles\n",
        "- 🌐 Malicious domains and phishing sites\n",
        "- 📧 Email addresses and breach correlations\n",
        "- 🚨 Comprehensive threat intelligence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-environment"
      },
      "outputs": [],
      "source": [
        "# 🚀 SETUP: Install dependencies for comprehensive analysis\n",
        "!pip install transformers torch torchvision torchaudio --quiet\n",
        "!pip install datasets accelerate evaluate --quiet\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn --quiet\n",
        "!pip install web3 eth-utils --quiet\n",
        "!pip install ipywidgets tqdm --quiet\n",
        "\n",
        "print(\"✅ All dependencies installed!\")\n",
        "print(\"🎯 Ready for comprehensive threat intelligence training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# 🔍 Check GPU availability and optimize for older hardware\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU optimization for older hardware\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🔥 Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    \n",
        "    # Optimize for older GPUs\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"🚀 Optimized for overnight training on older hardware\")\n",
        "else:\n",
        "    print(\"⚠️ No GPU detected - will use CPU (slower but works)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "# 📁 Mount Google Drive to access your datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directory structure\n",
        "import os\n",
        "os.makedirs('/content/datasets', exist_ok=True)\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "print(\"✅ Drive mounted and directories created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload-datasets"
      },
      "outputs": [],
      "source": [
        "# 📤 STEP 1: Upload your comprehensive threat intelligence dataset\n",
        "# Run this cell and upload your comprehensive_threat_intelligence.json file\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"🔄 Upload your comprehensive_threat_intelligence.json file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to datasets directory\n",
        "for filename, content in uploaded.items():\n",
        "    if filename.endswith('.json'):\n",
        "        shutil.move(filename, f'/content/datasets/{filename}')\n",
        "        print(f\"✅ Uploaded: {filename}\")\n",
        "\n",
        "print(\"\\n📊 Upload any additional datasets you have:\")\n",
        "print(\"- Ethereum scam dataset (CSV)\")\n",
        "print(\"- Additional threat intelligence files\")\n",
        "print(\"- Any blockchain transaction data\")\n",
        "\n",
        "# Optional: Upload additional files\n",
        "additional_files = files.upload()\n",
        "for filename, content in additional_files.items():\n",
        "    shutil.move(filename, f'/content/datasets/{filename}')\n",
        "    print(f\"✅ Additional file: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-comprehensive-data"
      },
      "outputs": [],
      "source": [
        "# 🔍 STEP 2: Load and analyze comprehensive threat intelligence\n",
        "\n",
        "def load_comprehensive_data():\n",
        "    \"\"\"Load all threat intelligence data\"\"\"\n",
        "    all_data = []\n",
        "    \n",
        "    # Load comprehensive threat intelligence\n",
        "    try:\n",
        "        with open('/content/datasets/comprehensive_threat_intelligence.json', 'r') as f:\n",
        "            comprehensive_data = json.load(f)\n",
        "            print(f\"✅ Loaded {len(comprehensive_data)} comprehensive threat records\")\n",
        "            all_data.extend(comprehensive_data)\n",
        "    except FileNotFoundError:\n",
        "        print(\"⚠️ comprehensive_threat_intelligence.json not found\")\n",
        "    \n",
        "    # Load additional datasets\n",
        "    dataset_files = os.listdir('/content/datasets')\n",
        "    \n",
        "    for file in dataset_files:\n",
        "        if file.endswith('.csv') and 'ethereum' in file.lower():\n",
        "            try:\n",
        "                df = pd.read_csv(f'/content/datasets/{file}')\n",
        "                print(f\"✅ Loaded CSV: {file} ({len(df)} records)\")\n",
        "                \n",
        "                # Convert CSV to threat intelligence format\n",
        "                for _, row in df.iterrows():\n",
        "                    record = {\n",
        "                        'type': 'blockchain_intelligence',\n",
        "                        'data': row.to_dict(),\n",
        "                        'timestamp': int(pd.Timestamp.now().timestamp())\n",
        "                    }\n",
        "                    all_data.append(record)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error loading {file}: {e}\")\n",
        "    \n",
        "    return all_data\n",
        "\n",
        "# Load all data\n",
        "threat_data = load_comprehensive_data()\n",
        "\n",
        "print(f\"\\n📊 COMPREHENSIVE DATASET LOADED\")\n",
        "print(f\"Total records: {len(threat_data)}\")\n",
        "\n",
        "# Analyze data distribution\n",
        "data_types = {}\n",
        "for record in threat_data:\n",
        "    data_type = record.get('type', 'unknown')\n",
        "    data_types[data_type] = data_types.get(data_type, 0) + 1\n",
        "\n",
        "print(\"\\n🎯 Data Distribution:\")\n",
        "for data_type, count in data_types.items():\n",
        "    print(f\"  {data_type}: {count} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare-training-data"
      },
      "source": [
        "# 🔄 STEP 3: Prepare unified training data for multi-modal analysis\n",
        "\n",
        "class ComprehensiveThreatDataset(Dataset):\n",
        "    \"\"\"Dataset for comprehensive threat intelligence\"\"\"\n",
        "    \n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Prepare labels and features\n",
        "        self.features = []\n",
        "        self.labels = []\n",
        "        \n",
        "        for record in data:\n",
        "            feature_text = self._extract_features(record)\n",
        "            label = self._determine_threat_level(record)\n",
        "            \n",
        "            self.features.append(feature_text)\n",
        "            self.labels.append(label)\n",
        "    \n",
        "    def _extract_features(self, record):\n",
        "        \"\"\"Extract text features from multi-modal data\"\"\"\n",
        "        data_type = record.get('type', '')\n",
        "        data_content = record.get('data', {})\n",
        "        \n",
        "        feature_parts = [f\"Type: {data_type}\"]\n",
        "        \n",
        "        if data_type == 'blockchain_intelligence':\n",
        "            address = data_content.get('address', '')\n",
        "            risk_indicators = data_content.get('risk_indicators', [])\n",
        "            feature_parts.append(f\"Address: {address}\")\n",
        "            if risk_indicators:\n",
        "                feature_parts.append(f\"Risks: {', '.join(risk_indicators)}\")\n",
        "        \n",
        "        elif data_type == 'username_intelligence':\n",
        "            username = data_content.get('username', '')\n",
        "            scam_reports = data_content.get('scam_reports', [])\n",
        "            feature_parts.append(f\"Username: {username}\")\n",
        "            if scam_reports:\n",
        "                patterns = [report.get('pattern', '') for report in scam_reports]\n",
        "                feature_parts.append(f\"Scam patterns: {', '.join(patterns)}\")\n",
        "        \n",
        "        elif data_type == 'domain_intelligence':\n",
        "            domain = data_content.get('domain', '')\n",
        "            reputation = data_content.get('reputation_score', 0)\n",
        "            phishing = data_content.get('phishing_indicators', [])\n",
        "            feature_parts.append(f\"Domain: {domain}\")\n",
        "            feature_parts.append(f\"Reputation: {reputation}\")\n",
        "            if phishing:\n",
        "                feature_parts.append(f\"Phishing indicators: {len(phishing)}\")\n",
        "        \n",
        "        elif data_type == 'email_intelligence':\n",
        "            email = data_content.get('email', '')\n",
        "            breaches = data_content.get('breach_history', [])\n",
        "            risks = data_content.get('risk_indicators', [])\n",
        "            feature_parts.append(f\"Email: {email}\")\n",
        "            feature_parts.append(f\"Breaches: {len(breaches)}\")\n",
        "            if risks:\n",
        "                feature_parts.append(f\"Risk indicators: {', '.join(risks)}\")\n",
        "        \n",
        "        return \" | \".join(feature_parts)\n",
        "    \n",
        "    def _determine_threat_level(self, record):\n",
        "        \"\"\"Determine threat level from multi-modal indicators\"\"\"\n",
        "        data_content = record.get('data', {})\n",
        "        data_type = record.get('type', '')\n",
        "        \n",
        "        # High threat indicators\n",
        "        high_threat_indicators = 0\n",
        "        \n",
        "        if data_type == 'username_intelligence':\n",
        "            scam_reports = data_content.get('scam_reports', [])\n",
        "            high_threat_indicators += len(scam_reports)\n",
        "        \n",
        "        elif data_type == 'domain_intelligence':\n",
        "            phishing = data_content.get('phishing_indicators', [])\n",
        "            reputation = data_content.get('reputation_score', 0)\n",
        "            high_threat_indicators += len(phishing)\n",
        "            if reputation > 0.5:\n",
        "                high_threat_indicators += 1\n",
        "        \n",
        "        elif data_type == 'email_intelligence':\n",
        "            risks = data_content.get('risk_indicators', [])\n",
        "            high_threat_indicators += len(risks)\n",
        "        \n",
        "        # Return threat level\n",
        "        if high_threat_indicators >= 2:\n",
        "            return 2  # High threat\n",
        "        elif high_threat_indicators >= 1:\n",
        "            return 1  # Medium threat\n",
        "        else:\n",
        "            return 0  # Low threat\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer (optimized for older hardware)\n",
        "model_name = \"distilbert-base-uncased\"  # Smaller, faster model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"✅ Tokenizer loaded: {model_name}\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = ComprehensiveThreatDataset(threat_data, tokenizer)\n",
        "print(f\"✅ Dataset created with {len(dataset)} samples\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"📊 Training samples: {len(train_dataset)}\")\n",
        "print(f\"📊 Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\n🔍 Sample feature:\")\n",
        "print(dataset.features[0][:200] + \"...\")\n",
        "print(f\"Label: {dataset.labels[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define-model"
      },
      "outputs": [],
      "source": [
        "# 🤖 STEP 4: Define Comprehensive Threat Intelligence Model\n",
        "\n",
        "class ComprehensiveThreatClassifier(nn.Module):\n",
        "    \"\"\"Multi-modal threat intelligence classifier\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, num_labels=3, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        # Multi-layer classifier for threat detection\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 4, num_labels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        \n",
        "        # Use pooled output\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        # Calculate loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "        \n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'logits': logits\n",
        "        }\n",
        "\n",
        "# Initialize model\n",
        "model = ComprehensiveThreatClassifier(model_name, num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"✅ Model initialized on {device}\")\n",
        "print(f\"📊 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"🎯 Threat levels: 0=Low, 1=Medium, 2=High\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-setup"
      },
      "outputs": [],
      "source": [
        "# ⚙️ STEP 5: Training Configuration (Optimized for overnight runs)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute comprehensive metrics\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments optimized for older hardware + overnight runs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/models/comprehensive-threat-intelligence',\n",
        "    num_train_epochs=20,  # More epochs for overnight training\n",
        "    per_device_train_batch_size=8,  # Smaller batch size for older GPUs\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,  # Effective batch size = 8*2 = 16\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=2e-5,\n",
        "    logging_dir='/content/logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,  # Save disk space\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,  # Better for older hardware\n",
        "    fp16=True,  # Mixed precision for speed\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=None  # Disable wandb to save resources\n",
        ")\n",
        "\n",
        "print(\"✅ Training configuration optimized for:\")\n",
        "print(\"   🌙 Overnight runs (20 epochs)\")\n",
        "print(\"   🔧 Older hardware compatibility\")\n",
        "print(\"   💾 Memory efficiency\")\n",
        "print(\"   🚀 Mixed precision training\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"🎯 Trainer initialized and ready for comprehensive threat intelligence training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-training"
      },
      "outputs": [],
      "source": [
        "# 🚀 STEP 6: START COMPREHENSIVE TRAINING (Perfect for overnight runs)\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"🚀 STARTING COMPREHENSIVE THREAT INTELLIGENCE TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"🕐 Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"🎯 Training on {len(train_dataset)} samples\")\n",
        "print(f\"🔍 Validating on {len(val_dataset)} samples\")\n",
        "print(f\"🌙 Optimized for overnight training (20 epochs)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clear cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Start training\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    training_results = trainer.train()\n",
        "    \n",
        "    # Training completed\n",
        "    end_time = time.time()\n",
        "    training_duration = end_time - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"⏰ Training duration: {training_duration/3600:.2f} hours\")\n",
        "    print(f\"📊 Final training loss: {training_results.training_loss:.4f}\")\n",
        "    print(f\"🕐 End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # Save final model\n",
        "    trainer.save_model('/content/models/final-comprehensive-threat-model')\n",
        "    tokenizer.save_pretrained('/content/models/final-comprehensive-threat-model')\n",
        "    \n",
        "    print(\"💾 Model saved successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Training error: {e}\")\n",
        "    print(\"💡 Try reducing batch size or model complexity\")\n",
        "    \n",
        "    # Save current progress\n",
        "    trainer.save_model('/content/models/interrupted-model')\n",
        "    print(\"💾 Progress saved for recovery\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate-model"
      },
      "outputs": [],
      "source": [
        "# 📊 STEP 7: Comprehensive Model Evaluation\n",
        "\n",
        "print(\"📊 EVALUATING COMPREHENSIVE THREAT INTELLIGENCE MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Evaluate on validation set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n🎯 FINAL EVALUATION RESULTS:\")\n",
        "print(f\"   Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"   F1-Score: {eval_results['eval_f1']:.4f}\")\n",
        "print(f\"   Precision: {eval_results['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {eval_results['eval_recall']:.4f}\")\n",
        "print(f\"   Loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# Test on sample data\n",
        "def test_sample_predictions():\n",
        "    \"\"\"Test model on sample threat indicators\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    test_samples = [\n",
        "        \"Type: username_intelligence | Username: @crypto_king_2024 | Scam patterns: crypto_king\",\n",
        "        \"Type: domain_intelligence | Domain: fake-binance.org | Reputation: 0.0 | Phishing indicators: 1\",\n",
        "        \"Type: email_intelligence | Email: help@metamask-support.com | Breaches: 0 | Risk indicators: \",\n",
        "        \"Type: blockchain_intelligence | Address: 0x1234567890123456789012345678901234567890 | Risks: \"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\n🧪 SAMPLE PREDICTIONS:\")\n",
        "    for i, sample in enumerate(test_samples):\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(sample, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = predictions[0][predicted_class].item()\n",
        "        \n",
        "        threat_levels = ['Low', 'Medium', 'High']\n",
        "        print(f\"   Sample {i+1}: {threat_levels[predicted_class]} threat (confidence: {confidence:.3f})\")\n",
        "        print(f\"      Input: {sample[:80]}...\")\n",
        "        print()\n",
        "\n",
        "test_sample_predictions()\n",
        "\n",
        "print(\"✅ Comprehensive threat intelligence model evaluation complete!\")\n",
        "print(\"🎯 Model can now analyze blockchain addresses, usernames, domains, and emails!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-model"
      },
      "outputs": [],
      "source": [
        "# 📦 STEP 8: Download Trained Model\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"📦 PREPARING MODEL FOR DOWNLOAD\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create zip file with model\n",
        "def create_model_zip():\n",
        "    \"\"\"Create downloadable model package\"\"\"\n",
        "    zip_path = '/content/comprehensive-threat-intelligence-model.zip'\n",
        "    \n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # Add model files\n",
        "        model_dir = '/content/models/final-comprehensive-threat-model'\n",
        "        if os.path.exists(model_dir):\n",
        "            for root, dirs, files in os.walk(model_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, '/content/models')\n",
        "                    zipf.write(file_path, arcname)\n",
        "        \n",
        "        # Add training logs if they exist\n",
        "        if os.path.exists('/content/logs'):\n",
        "            for root, dirs, files in os.walk('/content/logs'):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = f\"logs/{file}\"\n",
        "                    zipf.write(file_path, arcname)\n",
        "    \n",
        "    return zip_path\n",
        "\n",
        "# Create and download model\n",
        "zip_path = create_model_zip()\n",
        "\n",
        "print(f\"✅ Model packaged: {zip_path}\")\n",
        "print(f\"📊 Package size: {os.path.getsize(zip_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "# Download the model\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n",
        "\n",
        "print(\"\\n🎉 SUCCESS! Your comprehensive threat intelligence model is ready!\")\n",
        "print(\"\\n📋 WHAT YOU'VE BUILT:\")\n",
        "print(\"   ✅ Multi-modal threat intelligence classifier\")\n",
        "print(\"   ✅ Blockchain address analysis\")\n",
        "print(\"   ✅ Username/handle scam detection\")\n",
        "print(\"   ✅ Domain phishing detection\")\n",
        "print(\"   ✅ Email breach correlation\")\n",
        "print(\"   ✅ Unified threat scoring system\")\n",
        "\n",
        "print(\"\\n🚀 NEXT STEPS:\")\n",
        "print(\"   1. Integrate model into your Have I Been Rekt application\")\n",
        "print(\"   2. Test with real threat indicators\")\n",
        "print(\"   3. Collect more data to improve accuracy\")\n",
        "print(\"   4. Deploy as API for real-time threat analysis\")\n",
        "\n",
        "print(\"\\n🌟 Your comprehensive threat intelligence system is LIVE!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",\n    "colab": {\n      "gpuType": "T4",\n      "provenance": []\n    },\n    "kernelspec": {\n      "display_name": "Python 3",\n      "name": "python3"\n    },\n    "language_info": {\n      "name": "python"\n    }\n  },\n  "nbformat": 4,\n  "nbformat_minor": 0\n}