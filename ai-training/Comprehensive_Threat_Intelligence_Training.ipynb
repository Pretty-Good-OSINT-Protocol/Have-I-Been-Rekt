{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comprehensive-threat-header"
      },
      "source": [
        "# üî• Comprehensive Threat Intelligence AI Training\n",
        "## Multi-Modal Blockchain + OSINT Analysis\n",
        "\n",
        "**Optimized for overnight training runs** üåô\n",
        "\n",
        "This notebook trains an AI model on:\n",
        "- üîó Blockchain addresses and transaction patterns  \n",
        "- üë§ Usernames and social media handles\n",
        "- üåê Malicious domains and phishing sites\n",
        "- üìß Email addresses and breach correlations\n",
        "- üö® Comprehensive threat intelligence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-environment"
      },
      "outputs": [],
      "source": [
        "# üöÄ SETUP: Install dependencies for comprehensive analysis\n",
        "!pip install transformers torch torchvision torchaudio --quiet\n",
        "!pip install datasets accelerate evaluate --quiet\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn --quiet\n",
        "!pip install web3 eth-utils --quiet\n",
        "!pip install ipywidgets tqdm --quiet\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n",
        "print(\"üéØ Ready for comprehensive threat intelligence training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# üîç Check GPU availability and optimize for older hardware\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU optimization for older hardware\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üî• Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    \n",
        "    # Optimize for older GPUs\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"üöÄ Optimized for overnight training on older hardware\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected - will use CPU (slower but works)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "# üìÅ Mount Google Drive to access your datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directory structure\n",
        "import os\n",
        "os.makedirs('/content/datasets', exist_ok=True)\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "print(\"‚úÖ Drive mounted and directories created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload-datasets"
      },
      "outputs": [],
      "source": [
        "# üì§ STEP 1: Upload your comprehensive threat intelligence dataset\n",
        "# Run this cell and upload your comprehensive_threat_intelligence.json file\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"üîÑ Upload your comprehensive_threat_intelligence.json file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to datasets directory\n",
        "for filename, content in uploaded.items():\n",
        "    if filename.endswith('.json'):\n",
        "        shutil.move(filename, f'/content/datasets/{filename}')\n",
        "        print(f\"‚úÖ Uploaded: {filename}\")\n",
        "\n",
        "print(\"\\nüìä Upload any additional datasets you have:\")\n",
        "print(\"- Ethereum scam dataset (CSV)\")\n",
        "print(\"- Additional threat intelligence files\")\n",
        "print(\"- Any blockchain transaction data\")\n",
        "\n",
        "# Optional: Upload additional files\n",
        "additional_files = files.upload()\n",
        "for filename, content in additional_files.items():\n",
        "    shutil.move(filename, f'/content/datasets/{filename}')\n",
        "    print(f\"‚úÖ Additional file: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-comprehensive-data"
      },
      "outputs": [],
      "source": [
        "# üîç STEP 2: Load and analyze comprehensive threat intelligence\n",
        "\n",
        "def load_comprehensive_data():\n",
        "    \"\"\"Load all threat intelligence data\"\"\"\n",
        "    all_data = []\n",
        "    \n",
        "    # Load comprehensive threat intelligence\n",
        "    try:\n",
        "        with open('/content/datasets/comprehensive_threat_intelligence.json', 'r') as f:\n",
        "            comprehensive_data = json.load(f)\n",
        "            print(f\"‚úÖ Loaded {len(comprehensive_data)} comprehensive threat records\")\n",
        "            all_data.extend(comprehensive_data)\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è comprehensive_threat_intelligence.json not found\")\n",
        "    \n",
        "    # Load additional datasets\n",
        "    dataset_files = os.listdir('/content/datasets')\n",
        "    \n",
        "    for file in dataset_files:\n",
        "        if file.endswith('.csv') and 'ethereum' in file.lower():\n",
        "            try:\n",
        "                df = pd.read_csv(f'/content/datasets/{file}')\n",
        "                print(f\"‚úÖ Loaded CSV: {file} ({len(df)} records)\")\n",
        "                \n",
        "                # Convert CSV to threat intelligence format\n",
        "                for _, row in df.iterrows():\n",
        "                    record = {\n",
        "                        'type': 'blockchain_intelligence',\n",
        "                        'data': row.to_dict(),\n",
        "                        'timestamp': int(pd.Timestamp.now().timestamp())\n",
        "                    }\n",
        "                    all_data.append(record)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading {file}: {e}\")\n",
        "    \n",
        "    return all_data\n",
        "\n",
        "# Load all data\n",
        "threat_data = load_comprehensive_data()\n",
        "\n",
        "print(f\"\\nüìä COMPREHENSIVE DATASET LOADED\")\n",
        "print(f\"Total records: {len(threat_data)}\")\n",
        "\n",
        "# Analyze data distribution\n",
        "data_types = {}\n",
        "for record in threat_data:\n",
        "    data_type = record.get('type', 'unknown')\n",
        "    data_types[data_type] = data_types.get(data_type, 0) + 1\n",
        "\n",
        "print(\"\\nüéØ Data Distribution:\")\n",
        "for data_type, count in data_types.items():\n",
        "    print(f\"  {data_type}: {count} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare-training-data"
      },
      "source": [
        "# üîÑ STEP 3: Prepare unified training data for multi-modal analysis\n",
        "\n",
        "class ComprehensiveThreatDataset(Dataset):\n",
        "    \"\"\"Dataset for comprehensive threat intelligence\"\"\"\n",
        "    \n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Prepare labels and features\n",
        "        self.features = []\n",
        "        self.labels = []\n",
        "        \n",
        "        for record in data:\n",
        "            feature_text = self._extract_features(record)\n",
        "            label = self._determine_threat_level(record)\n",
        "            \n",
        "            self.features.append(feature_text)\n",
        "            self.labels.append(label)\n",
        "    \n",
        "    def _extract_features(self, record):\n",
        "        \"\"\"Extract text features from multi-modal data\"\"\"\n",
        "        data_type = record.get('type', '')\n",
        "        data_content = record.get('data', {})\n",
        "        \n",
        "        feature_parts = [f\"Type: {data_type}\"]\n",
        "        \n",
        "        if data_type == 'blockchain_intelligence':\n",
        "            address = data_content.get('address', '')\n",
        "            risk_indicators = data_content.get('risk_indicators', [])\n",
        "            feature_parts.append(f\"Address: {address}\")\n",
        "            if risk_indicators:\n",
        "                feature_parts.append(f\"Risks: {', '.join(risk_indicators)}\")\n",
        "        \n",
        "        elif data_type == 'username_intelligence':\n",
        "            username = data_content.get('username', '')\n",
        "            scam_reports = data_content.get('scam_reports', [])\n",
        "            feature_parts.append(f\"Username: {username}\")\n",
        "            if scam_reports:\n",
        "                patterns = [report.get('pattern', '') for report in scam_reports]\n",
        "                feature_parts.append(f\"Scam patterns: {', '.join(patterns)}\")\n",
        "        \n",
        "        elif data_type == 'domain_intelligence':\n",
        "            domain = data_content.get('domain', '')\n",
        "            reputation = data_content.get('reputation_score', 0)\n",
        "            phishing = data_content.get('phishing_indicators', [])\n",
        "            feature_parts.append(f\"Domain: {domain}\")\n",
        "            feature_parts.append(f\"Reputation: {reputation}\")\n",
        "            if phishing:\n",
        "                feature_parts.append(f\"Phishing indicators: {len(phishing)}\")\n",
        "        \n",
        "        elif data_type == 'email_intelligence':\n",
        "            email = data_content.get('email', '')\n",
        "            breaches = data_content.get('breach_history', [])\n",
        "            risks = data_content.get('risk_indicators', [])\n",
        "            feature_parts.append(f\"Email: {email}\")\n",
        "            feature_parts.append(f\"Breaches: {len(breaches)}\")\n",
        "            if risks:\n",
        "                feature_parts.append(f\"Risk indicators: {', '.join(risks)}\")\n",
        "        \n",
        "        return \" | \".join(feature_parts)\n",
        "    \n",
        "    def _determine_threat_level(self, record):\n",
        "        \"\"\"Determine threat level from multi-modal indicators\"\"\"\n",
        "        data_content = record.get('data', {})\n",
        "        data_type = record.get('type', '')\n",
        "        \n",
        "        # High threat indicators\n",
        "        high_threat_indicators = 0\n",
        "        \n",
        "        if data_type == 'username_intelligence':\n",
        "            scam_reports = data_content.get('scam_reports', [])\n",
        "            high_threat_indicators += len(scam_reports)\n",
        "        \n",
        "        elif data_type == 'domain_intelligence':\n",
        "            phishing = data_content.get('phishing_indicators', [])\n",
        "            reputation = data_content.get('reputation_score', 0)\n",
        "            high_threat_indicators += len(phishing)\n",
        "            if reputation > 0.5:\n",
        "                high_threat_indicators += 1\n",
        "        \n",
        "        elif data_type == 'email_intelligence':\n",
        "            risks = data_content.get('risk_indicators', [])\n",
        "            high_threat_indicators += len(risks)\n",
        "        \n",
        "        # Return threat level\n",
        "        if high_threat_indicators >= 2:\n",
        "            return 2  # High threat\n",
        "        elif high_threat_indicators >= 1:\n",
        "            return 1  # Medium threat\n",
        "        else:\n",
        "            return 0  # Low threat\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer (optimized for older hardware)\n",
        "model_name = \"distilbert-base-uncased\"  # Smaller, faster model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"‚úÖ Tokenizer loaded: {model_name}\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = ComprehensiveThreatDataset(threat_data, tokenizer)\n",
        "print(f\"‚úÖ Dataset created with {len(dataset)} samples\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"üìä Training samples: {len(train_dataset)}\")\n",
        "print(f\"üìä Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nüîç Sample feature:\")\n",
        "print(dataset.features[0][:200] + \"...\")\n",
        "print(f\"Label: {dataset.labels[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define-model"
      },
      "outputs": [],
      "source": [
        "# ü§ñ STEP 4: Define Comprehensive Threat Intelligence Model\n",
        "\n",
        "class ComprehensiveThreatClassifier(nn.Module):\n",
        "    \"\"\"Multi-modal threat intelligence classifier\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, num_labels=3, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        # Multi-layer classifier for threat detection\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 4, num_labels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        \n",
        "        # Use pooled output\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        # Calculate loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "        \n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'logits': logits\n",
        "        }\n",
        "\n",
        "# Initialize model\n",
        "model = ComprehensiveThreatClassifier(model_name, num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Model initialized on {device}\")\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"üéØ Threat levels: 0=Low, 1=Medium, 2=High\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-setup"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è STEP 5: Training Configuration (Optimized for overnight runs)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute comprehensive metrics\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments optimized for older hardware + overnight runs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/models/comprehensive-threat-intelligence',\n",
        "    num_train_epochs=20,  # More epochs for overnight training\n",
        "    per_device_train_batch_size=8,  # Smaller batch size for older GPUs\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,  # Effective batch size = 8*2 = 16\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=2e-5,\n",
        "    logging_dir='/content/logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,  # Save disk space\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,  # Better for older hardware\n",
        "    fp16=True,  # Mixed precision for speed\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=None  # Disable wandb to save resources\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration optimized for:\")\n",
        "print(\"   üåô Overnight runs (20 epochs)\")\n",
        "print(\"   üîß Older hardware compatibility\")\n",
        "print(\"   üíæ Memory efficiency\")\n",
        "print(\"   üöÄ Mixed precision training\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"üéØ Trainer initialized and ready for comprehensive threat intelligence training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-training"
      },
      "outputs": [],
      "source": [
        "# üöÄ STEP 6: START COMPREHENSIVE TRAINING (Perfect for overnight runs)\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üöÄ STARTING COMPREHENSIVE THREAT INTELLIGENCE TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üïê Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üéØ Training on {len(train_dataset)} samples\")\n",
        "print(f\"üîç Validating on {len(val_dataset)} samples\")\n",
        "print(f\"üåô Optimized for overnight training (20 epochs)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clear cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Start training\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    training_results = trainer.train()\n",
        "    \n",
        "    # Training completed\n",
        "    end_time = time.time()\n",
        "    training_duration = end_time - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"‚è∞ Training duration: {training_duration/3600:.2f} hours\")\n",
        "    print(f\"üìä Final training loss: {training_results.training_loss:.4f}\")\n",
        "    print(f\"üïê End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # Save final model\n",
        "    trainer.save_model('/content/models/final-comprehensive-threat-model')\n",
        "    tokenizer.save_pretrained('/content/models/final-comprehensive-threat-model')\n",
        "    \n",
        "    print(\"üíæ Model saved successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training error: {e}\")\n",
        "    print(\"üí° Try reducing batch size or model complexity\")\n",
        "    \n",
        "    # Save current progress\n",
        "    trainer.save_model('/content/models/interrupted-model')\n",
        "    print(\"üíæ Progress saved for recovery\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate-model"
      },
      "outputs": [],
      "source": [
        "# üìä STEP 7: Comprehensive Model Evaluation\n",
        "\n",
        "print(\"üìä EVALUATING COMPREHENSIVE THREAT INTELLIGENCE MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Evaluate on validation set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nüéØ FINAL EVALUATION RESULTS:\")\n",
        "print(f\"   Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"   F1-Score: {eval_results['eval_f1']:.4f}\")\n",
        "print(f\"   Precision: {eval_results['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {eval_results['eval_recall']:.4f}\")\n",
        "print(f\"   Loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# Test on sample data\n",
        "def test_sample_predictions():\n",
        "    \"\"\"Test model on sample threat indicators\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    test_samples = [\n",
        "        \"Type: username_intelligence | Username: @crypto_king_2024 | Scam patterns: crypto_king\",\n",
        "        \"Type: domain_intelligence | Domain: fake-binance.org | Reputation: 0.0 | Phishing indicators: 1\",\n",
        "        \"Type: email_intelligence | Email: help@metamask-support.com | Breaches: 0 | Risk indicators: \",\n",
        "        \"Type: blockchain_intelligence | Address: 0x1234567890123456789012345678901234567890 | Risks: \"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nüß™ SAMPLE PREDICTIONS:\")\n",
        "    for i, sample in enumerate(test_samples):\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(sample, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = predictions[0][predicted_class].item()\n",
        "        \n",
        "        threat_levels = ['Low', 'Medium', 'High']\n",
        "        print(f\"   Sample {i+1}: {threat_levels[predicted_class]} threat (confidence: {confidence:.3f})\")\n",
        "        print(f\"      Input: {sample[:80]}...\")\n",
        "        print()\n",
        "\n",
        "test_sample_predictions()\n",
        "\n",
        "print(\"‚úÖ Comprehensive threat intelligence model evaluation complete!\")\n",
        "print(\"üéØ Model can now analyze blockchain addresses, usernames, domains, and emails!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-model"
      },
      "outputs": [],
      "source": [
        "# üì¶ STEP 8: Download Trained Model\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"üì¶ PREPARING MODEL FOR DOWNLOAD\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create zip file with model\n",
        "def create_model_zip():\n",
        "    \"\"\"Create downloadable model package\"\"\"\n",
        "    zip_path = '/content/comprehensive-threat-intelligence-model.zip'\n",
        "    \n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # Add model files\n",
        "        model_dir = '/content/models/final-comprehensive-threat-model'\n",
        "        if os.path.exists(model_dir):\n",
        "            for root, dirs, files in os.walk(model_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, '/content/models')\n",
        "                    zipf.write(file_path, arcname)\n",
        "        \n",
        "        # Add training logs if they exist\n",
        "        if os.path.exists('/content/logs'):\n",
        "            for root, dirs, files in os.walk('/content/logs'):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = f\"logs/{file}\"\n",
        "                    zipf.write(file_path, arcname)\n",
        "    \n",
        "    return zip_path\n",
        "\n",
        "# Create and download model\n",
        "zip_path = create_model_zip()\n",
        "\n",
        "print(f\"‚úÖ Model packaged: {zip_path}\")\n",
        "print(f\"üìä Package size: {os.path.getsize(zip_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "# Download the model\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n",
        "\n",
        "print(\"\\nüéâ SUCCESS! Your comprehensive threat intelligence model is ready!\")\n",
        "print(\"\\nüìã WHAT YOU'VE BUILT:\")\n",
        "print(\"   ‚úÖ Multi-modal threat intelligence classifier\")\n",
        "print(\"   ‚úÖ Blockchain address analysis\")\n",
        "print(\"   ‚úÖ Username/handle scam detection\")\n",
        "print(\"   ‚úÖ Domain phishing detection\")\n",
        "print(\"   ‚úÖ Email breach correlation\")\n",
        "print(\"   ‚úÖ Unified threat scoring system\")\n",
        "\n",
        "print(\"\\nüöÄ NEXT STEPS:\")\n",
        "print(\"   1. Integrate model into your Have I Been Rekt application\")\n",
        "print(\"   2. Test with real threat indicators\")\n",
        "print(\"   3. Collect more data to improve accuracy\")\n",
        "print(\"   4. Deploy as API for real-time threat analysis\")\n",
        "\n",
        "print(\"\\nüåü Your comprehensive threat intelligence system is LIVE!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",\n    "colab": {\n      "gpuType": "T4",\n      "provenance": []\n    },\n    "kernelspec": {\n      "display_name": "Python 3",\n      "name": "python3"\n    },\n    "language_info": {\n      "name": "python"\n    }\n  },\n  "nbformat": 4,\n  "nbformat_minor": 0\n}